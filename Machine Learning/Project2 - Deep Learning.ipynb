{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ef3645b",
   "metadata": {},
   "source": [
    "# PROJECT 2 - DEEP LEARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1af485",
   "metadata": {},
   "source": [
    "## THE MNIST DATADASE\n",
    "\n",
    "Uma rede neural treinada para reconhecer imagens e problemas númericos escritos à mão.  \n",
    "\n",
    "- Pode ser usado para interpretar um CEP de uma carta escrita manualmente\n",
    "automaticamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75347c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as nf\n",
    "#library for computation vision\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c2042e",
   "metadata": {},
   "source": [
    "## Transform dataset(images) for Tensor file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3db9e666",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copied MNIST database on site them\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "#Carrega a parte de treino do dataset\n",
    "trainset = datasets.MNIST('./MNIST_data/', \n",
    "                          download = True,\n",
    "                          train = True,\n",
    "                          transform = transform\n",
    "                          )\n",
    "\n",
    "#Cria um buffer para coletar os dados por partes\n",
    "trainloader = torch.utils.data.DataLoader(trainset,\n",
    "                                          batch_size = 64,\n",
    "                                          shuffle = True\n",
    "                                          )\n",
    "\n",
    "#Carrega os pacotes de validação do dataset\n",
    "valida_set = datasets.MNIST('./MNIST_data',\n",
    "                        download = True,\n",
    "                        train = False,\n",
    "                        transform = transform\n",
    "                        )\n",
    "\n",
    "#Cria um buffer para pegar os dados por partes\n",
    "valida_loader = torch.utils.data.DataLoader(valida_set,\n",
    "                                            batch_size = 64,\n",
    "                                            shuffle = True\n",
    "                                            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4991a289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Conferir a estrutura de dados representa a imagem corretamente\n",
    "dataiter = iter(trainloader)\n",
    "imagens, etiquetas = dataiter.next()\n",
    "plt.imshow(imagens[0].numpy().squeeze(),\n",
    "           cmap = 'gray_r');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "410a48e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "#Conferir o tamanho da imagem e etiqueta (dimensões do Tensor)\n",
    "print(imagens[0].shape)\n",
    "print(etiquetas[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb40159",
   "metadata": {},
   "source": [
    "## KERAS\n",
    "Keras é uma API projetada para seres humanos, não para máquinas. O Keras segue as melhores práticas para reduzir a carga cognitiva: oferece APIs simples e consistentes, minimiza o número de ações do usuário necessárias para casos de uso comuns e fornece mensagens de erro claras e acionáveis. Ele também possui extensa documentação e guias para desenvolvedores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41386a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network - \n",
    "# Keras - inceptionv3 (herança: copiando as camadas da rede)\n",
    "class Modelo(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Camadas de entrada e interna\n",
    "        - Usando uma função linear que ativa as camadas internas da rede.\n",
    "        --------\n",
    "        Possibilidade de alterar a quantidade de neurônios,\n",
    "        porém quando maior a quantidade mais processamento é exigido.\n",
    "        \"\"\"\n",
    "        super(Modelo, self).__init__()\n",
    "        #Comada de entrada, 784 neurônios que se ligam a 128.\n",
    "        self.linear1 = nn.Linear(28*28, 128)\n",
    "        #Camada interna 1: 128 neurônios -> 64 neurônios.\n",
    "        self.linear2 = nn.Linear(128, 64)\n",
    "        #Camada interna 2: 64 neurônios -> 10 neurônios.\n",
    "        self.linear3 = nn.Linear(64, 10)\n",
    "        #Não ha necessidade de definir uma camada de saída, porque \n",
    "        #usamos o output da camada interna 2.\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Funções de ativação das camadas e retornando a perda atráves da função softmax.\"\"\"\n",
    "        x = F.relu(self.linear1(x))     #Camada de entrada => interna 1\n",
    "        x = F.relu(self.linear2(x))     #Interna 1 => interna 2\n",
    "        x = self.linear3(x)             #Interna 2 => saída (f(x) = x)\n",
    "        return F.log_softmax(x, dim=1)  #Dados utilizados para calcular a perda \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fff431",
   "metadata": {},
   "source": [
    "## TREINAMENTO / OTIMIZAÇÃO\n",
    "\n",
    "- Estrutura de treinamento da rede\n",
    "- Número de épocas do treinamento é 10.\n",
    "\n",
    "## Parâmetros de Bias\n",
    "A tradução de bias para o português é viés. \n",
    "Os significados mais relevantes para um contexto de dados são o de tendência e distorção.\n",
    "\n",
    "**Na estatística:** É o de desvio sistemático do valor real. É desvio por não coincidir com o valor real e é sistemático por acontecer consistentemente, em média.\n",
    "\n",
    "**Em Ciências de Dados:** O viés também é chamado de erro sistemático, pois ele é sobre o algoritmo não ser capaz de expressar a forma funcional com que aquele fenômeno se dá. Ou seja, por mais que usemos amostras diferentes ou maiores, ele nunca se aproximará do valor real que queremos estimar, pois não é capaz.\n",
    "\n",
    "**Referência:** [Bias em Ciência de Dados - Luis Moneda](http://lgmoneda.github.io/2019/01/14/bias-data-science.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59107a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def treino(modelo, trainloader, device):\n",
    "    \"\"\"Estrutura de treinamento da rede neural.\"\"\"\n",
    "    #-------------------------------------------------------------------\n",
    "    #Parâmetros de Bias\n",
    "    #Define a política de atualização dos pesos e da blas\n",
    "    otimizador = optim.SGD(modelo.parameters(), lr=0.01, momentun=0.5)\n",
    "    #Timer para duração do treino\n",
    "    inicio = time()\n",
    "    #-------------------------------------------------------------------\n",
    "    #Definindo o critério para calcular a perda\n",
    "    criterio = nn.NLLLoss()\n",
    "    #Número de épocas que o algoritmo rodará\n",
    "    #Ideal > 100 épocas\n",
    "    EPOCHS = 10\n",
    "    #Ativação do modo de treinamento do modelo\n",
    "    modelo.train()\n",
    "    #-------------------------------------------------------------------\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        #Iniciação da perda acumulada\n",
    "        perda_acumulada = 0\n",
    "        \n",
    "        for imagens, etiquetas in trainloader:\n",
    "            #-----------------------------------------------------------\n",
    "            #Convertendo imagens para vetores\n",
    "            imagens = imagens.view(imagens.shape[0], -1)\n",
    "            #Zerando os gradientes por conta do ciclo anterior\n",
    "            otimizador = zero_grad()\n",
    "            #-----------------------------------------------------------\n",
    "            #Colocando os dados no modelo\n",
    "            output = modelo(imagens.to(device))\n",
    "            #Calculando a perda da época em questão\n",
    "            perda_instantanea = criterio(output, etiquetas.to(device))\n",
    "            #-----------------------------------------------------------\n",
    "            #Back propagation a partir da perda\n",
    "            perda_instantanea.backward()\n",
    "            #-----------------------------------------------------------\n",
    "            #Atualizando os pesos e a bias\n",
    "            otimizador.step()\n",
    "            #-----------------------------------------------------------\n",
    "            #Atualização da perda acumulada\n",
    "            perda_acumulada += perda_instantanea.item()\n",
    "            #-----------------------------------------------------------\n",
    "            \n",
    "        else:\n",
    "            print(f'Epoch: {epoch+1}')\n",
    "            print(f' Perda resultante: {perda_acumulada/len(trainloader)}')\n",
    "    \n",
    "    print('------------')\n",
    "    print(f'Tempo do treinamento: {(time() - inicio/60)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35f08cf",
   "metadata": {},
   "source": [
    "## VALIDAÇÃO DO CÓDIGO\n",
    "- Calculando o valor de accuracy\n",
    "- Modelo implementado pelo Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27c46e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validacao(modelo, valida_loader, device):\n",
    "    \"\"\"Verificação da base de dados do treino em relação ao treinamento.\"\"\"\n",
    "    conta_correta, conta_all = 0, 0\n",
    "    \n",
    "    for imagens, etiquetas in valida_loader:\n",
    "        for i in range(len(etiquetas)):\n",
    "            img = image[i].view(1, 784)\n",
    "            #Desativação do autograd para acelerar a validação\n",
    "            #Grafos computacionais dinâmicos possuem um alto custo de processamento\n",
    "            with torch.no_grad():\n",
    "                #Output do modelo em escala logarítmica\n",
    "                logp = modelo(img.to(device))\n",
    "            #----------------------------------------------------------------\n",
    "            #Conversão para escola \"normal\" = Tensor    \n",
    "            ps = torch.exp(logp)\n",
    "            probab = list(ps.cpu().numpy()[0])\n",
    "            #Converte um tensor em um número que o modelo previu como correto\n",
    "            etiquetas_pred = probab.index(max(probab))\n",
    "            etiquetas_certa = etiquetas.numpy()[1]\n",
    "            #---------------------------------------------------------------\n",
    "            #Comparando a previsão com o valor correto\n",
    "            if (etiquetas_certa == etiquetas_pred):\n",
    "                conta_correta +=1\n",
    "            conta_all += 1\n",
    "            #---------------------------------------------------------------\n",
    "        \n",
    "        \n",
    "        print(f'Total de imagens testadas: {conta_all}')\n",
    "        print(f'Precisão do modelo: {(conta_correta*100)/conta_all}')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b00bec5",
   "metadata": {},
   "source": [
    "## EXECUÇÃO DO MODELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c878884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Modelo(\n",
       "  (linear1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (linear2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (linear3): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelo = Modelo()\n",
    "#Verificando se o CUDA está disponível na GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else \"cpu\")\n",
    "modelo.to(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
